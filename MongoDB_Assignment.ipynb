{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Theoretical questions:\n"
      ],
      "metadata": {
        "id": "OfrUvUTRRk99"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1. What are the key differences between SQL and NoSQL databases?\n",
        "\n",
        "Ans) SQL databases are relational and structured, using tables with fixed schemas (like MySQL or PostgreSQL).\n",
        "NoSQL databases such as MongoDB are non-relational and store data in flexible, JSON-like documents.\n",
        "SQL focuses on ACID compliance and complex joins, while NoSQL prioritizes scalability, flexibility, and performance for unstructured data.\n",
        "\n",
        "###2. What makes MongoDB a good choice for modern applications?\n",
        "\n",
        "Ans) MongoDB is a great choice for modern applications because it offers flexible data models, easy scalability, and high performance.\n",
        "It stores data in BSON (a binary form of JSON), integrates smoothly with modern programming languages, and supports distributed systems for handling large data volumes efficiently.\n",
        "\n",
        "###3. Explain the concept of collections in MongoDB.\n",
        "\n",
        "Ans) In MongoDB, a collection is like a table in SQL. It stores multiple documents that share similar structures but can have different fields.\n",
        "Collections allow flexible schema design, meaning documents can evolve without altering the database structure.\n",
        "\n",
        "###4. How does MongoDB ensure high availability using replication?\n",
        "\n",
        "Ans) MongoDB ensures high availability through replica sets.\n",
        "A replica set consists of a primary node (handles writes) and secondary nodes (replicate data from the primary).\n",
        "If the primary fails, a secondary node is automatically promoted to primary, ensuring continuous uptime and data availability.\n",
        "\n",
        "###5. What are the main benefits of MongoDB Atlas?\n",
        "\n",
        "MongoDB Atlas is a fully managed cloud version of MongoDB.\n",
        "It provides automatic scaling, real-time performance monitoring, backups, and strong security controls.\n",
        "It also simplifies deployment across AWS, Azure, and Google Cloud with minimal maintenance."
      ],
      "metadata": {
        "id": "SsGSAWzSRuUz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###6. What is the role of indexes in MongoDB, and how do they improve performance?\n",
        "\n",
        "Ans) Indexes in MongoDB help the database find and retrieve data faster.\n",
        "Without indexes, MongoDB scans every document in a collection (a full collection scan).\n",
        "Indexes, like single-field or compound indexes, significantly improve query performance but may slightly impact write speed.\n",
        "\n",
        "###7. Describe the stages of the MongoDB aggregation pipeline.\n",
        "\n",
        "Ans)The aggregation pipeline processes data through multiple stages, transforming documents at each step.\n",
        "Common stages include:\n",
        "\n",
        "$match → filters documents\n",
        "\n",
        "$group → groups data and performs aggregations\n",
        "\n",
        "$sort → sorts documents\n",
        "\n",
        "$project → selects or reshapes fields\n",
        "\n",
        "$limit / $skip → controls the number of results\n",
        "This enables complex data analysis directly within MongoDB.\n",
        "\n",
        "###8. What is sharding in MongoDB? How does it differ from replication?\n",
        "\n",
        "Ans) Sharding distributes data across multiple servers (called shards) to improve scalability and performance for large datasets.\n",
        "Replication, on the other hand, duplicates the same data across multiple servers for fault tolerance.\n",
        "In short, sharding = scalability; replication = reliability.\n",
        "\n",
        "###9. What is PyMongo, and why is it used?\n",
        "\n",
        "Ans) PyMongo is the official Python driver for MongoDB.\n",
        "It allows Python applications to connect to MongoDB, perform CRUD operations, run queries, and use aggregation pipelines.\n",
        "PyMongo makes it simple to integrate MongoDB with Python-based projects.\n",
        "\n",
        "###10. What are the ACID properties in the context of MongoDB transactions?\n",
        "\n",
        "Ans) ACID stands for Atomicity, Consistency, Isolation, and Durability.\n",
        "MongoDB supports multi-document ACID transactions, ensuring:\n",
        "\n",
        "Atomicity : all operations succeed or none do\n",
        "\n",
        "Consistency : database remains valid\n",
        "\n",
        "Isolation : transactions don’t interfere\n",
        "\n",
        "Durability : changes persist even after a crash\n",
        "\n",
        "###11. What is the purpose of MongoDB’s explain() function?\n",
        "\n",
        "Ans)The explain() function shows how MongoDB executes a query.\n",
        "It provides details like index usage, query plan, and performance metrics.\n",
        "Developers use it to analyze and optimize queries for better efficiency.\n",
        "\n",
        "###12. How does MongoDB handle schema validation?\n",
        "\n",
        "Ans) MongoDB supports optional schema validation rules.\n",
        "You can define a schema using JSON Schema syntax to enforce field types, required fields, and constraints.\n",
        "This ensures data integrity while maintaining flexibility for evolving data structures.\n",
        "\n",
        "###13. What is the difference between a primary and a secondary node in a replica set?\n",
        "\n",
        "Ans)In a replica set:\n",
        "\n",
        "The primary node handles all write operations and by default, reads.\n",
        "\n",
        "Secondary nodes replicate data from the primary and can serve read queries if enabled.\n",
        "If the primary fails, one of the secondaries becomes the new primary automatically.\n",
        "\n",
        "###14. What security mechanisms does MongoDB provide for data protection?\n",
        "\n",
        "Ans)MongoDB offers several security features:\n",
        "\n",
        "Authentication (verifying user identity)\n",
        "\n",
        "Authorization (role-based access control)\n",
        "\n",
        "Encryption (data encrypted in-transit and at-rest)\n",
        "\n",
        "Auditing (tracking user actions)\n",
        "These mechanisms ensure strong data protection and compliance.\n",
        "\n",
        "###15. Explain the concept of embedded documents and when they should be used.\n",
        "\n",
        "Ans)Embedded documents are nested JSON-like structures stored within a single MongoDB document.\n",
        "They are used when related data is frequently accessed together (e.g., user details and address).\n",
        "Embedding improves read performance and keeps related information in one place, avoiding complex joins."
      ],
      "metadata": {
        "id": "Er7h5JO8Vhmm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###16. What is the purpose of MongoDB’s $lookup stage in aggregation?\n",
        "\n",
        "Ans) The $lookup stage in MongoDB’s aggregation pipeline is used to perform a left outer join between two collections.\n",
        "It allows combining documents from different collections based on a shared field.\n",
        "\n",
        "For example, you can use $lookup to combine customer data with their order details.\n",
        "\n",
        "This stage is useful for enriching data without needing multiple queries.\n",
        "\n",
        "###17. What are some common use cases for MongoDB?\n",
        "\n",
        "Ans)MongoDB is widely used in scenarios where flexible and scalable data storage is needed.\n",
        "Common use cases include:\n",
        "\n",
        "Content management systems (CMS)\n",
        "\n",
        "E-commerce and product catalogs\n",
        "\n",
        "Real-time analytics\n",
        "\n",
        "IoT data storage\n",
        "\n",
        "Mobile and web applications\n",
        "\n",
        "Social media and user profiles\n",
        "Its document model makes it ideal for dynamic, fast-changing data.\n",
        "\n",
        "###18. What are the advantages of using MongoDB for horizontal scaling?\n",
        "\n",
        "Ans)MongoDB supports horizontal scaling through sharding, which distributes data across multiple servers.\n",
        "Advantages include:\n",
        "\n",
        "Better performance for large datasets\n",
        "\n",
        "Improved read and write throughput\n",
        "\n",
        "Ability to scale storage capacity easily\n",
        "\n",
        "Reduced load on individual servers\n",
        "This ensures that MongoDB can handle growing data and user demands efficiently.\n",
        "\n",
        "###19. How do MongoDB transactions differ from SQL transactions?\n",
        "\n",
        "Ans)MongoDB transactions are document-based and can span multiple documents or collections (since version 4.0).\n",
        "SQL transactions are table-based and typically enforce stricter schema and consistency.\n",
        "\n",
        "While SQL is inherently ACID-compliant, MongoDB adds ACID compliance at the document and multi-document level.\n",
        "MongoDB transactions are designed to be lightweight and optimized for distributed environments.\n",
        "\n",
        "###20. What are the main differences between capped collections and regular collections?\n",
        "\n",
        "Ans)Capped collections have a fixed size and automatically overwrite the oldest data when the limit is reached.\n",
        "\n",
        "They maintain insertion order and are ideal for logging or caching where only recent data matters.\n",
        "\n",
        "Regular collections grow dynamically and do not overwrite older data.\n",
        "Capped collections provide predictable performance due to their fixed size.\n",
        "\n",
        "###21). What is the purpose of the $match stage in MongoDB’s aggregation pipeline?\n",
        "\n",
        "Ans)The \"match\" stage filters documents based on specified conditions, similar to a SQL WHERE clause.\n",
        "It selects only documents that meet the criteria for further processing in the pipeline.\n",
        "\n",
        "Placing $match early in the pipeline improves performance by reducing the number of documents processed later.\n",
        "\n",
        "###22. How can you secure access to a MongoDB database?\n",
        "\n",
        "Ans)MongoDB security can be ensured using several mechanisms:\n",
        "\n",
        "Enable authentication to verify users.\n",
        "\n",
        "Use authorization with role-based access control (RBAC).\n",
        "\n",
        "Encrypt data both in transit (TLS/SSL) and at rest.\n",
        "\n",
        "Restrict network access using firewalls or IP whitelisting.\n",
        "\n",
        "Audit logs to monitor database activities.\n",
        "Following these practices helps protect MongoDB from unauthorized access and attacks.\n",
        "\n",
        "###23. What is MongoDB’s WiredTiger storage engine, and why is it important?\n",
        "\n",
        "Ans)WiredTiger is the default storage engine in MongoDB, designed for high performance and concurrency.\n",
        "\n",
        "It supports document-level locking, compression, and checkpointing for efficient data management.\n",
        "\n",
        "WiredTiger improves write throughput, reduces disk space usage, and provides durability through journaling.\n",
        "Its design makes MongoDB faster and more reliable for modern workloads."
      ],
      "metadata": {
        "id": "NozWrB_JXDvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical Questions"
      ],
      "metadata": {
        "id": "qSSHi-P0Y6fo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note: MongoDB Atlas has deprecated the classic M0 Free Cluster.So, i am just writing the valid queries. (Not sure this will be accepted or not):**"
      ],
      "metadata": {
        "id": "8OP1kXn0r5Mv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1. Write a Python script to load the Superstore dataset from a CSV file into MongoDB"
      ],
      "metadata": {
        "id": "yvS6erK3ZYKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pymongo import MongoClient\n",
        "\n",
        "# Connect to MongoDB\n",
        "client = MongoClient(\"mongodb://localhost:27017/\")\n",
        "db = client[\"SuperstoreDB\"]\n",
        "collection = db[\"Orders\"]\n",
        "\n",
        "# Load CSV file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "3FShUsqyX2xW",
        "outputId": "00f29ac3-f00a-4acc-deff-9f9de7997b5c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1652d170-f472-4d33-8e6d-6d46db154de2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1652d170-f472-4d33-8e6d-6d46db154de2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving superstore.csv to superstore.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"superstore.csv\", encoding=\"latin1\")\n",
        "\n",
        "print(\"Data inserted successfully into MongoDB!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjLH4JLAdZVJ",
        "outputId": "bc115388-84f7-484b-ac47-5e484677bdb0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data inserted successfully into MongoDB!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Retrieve and Print All Documents from the Orders Collection"
      ],
      "metadata": {
        "id": "T76HNVTBoYzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for order in collection.find():\n",
        "    print(order)"
      ],
      "metadata": {
        "id": "zEezp_owoeZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Count and Display Total Number of Documents"
      ],
      "metadata": {
        "id": "Aivaoug8s4vQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_docs = collection.count_documents({})\n",
        "print(\"Total number of documents:\", total_docs)\n"
      ],
      "metadata": {
        "id": "RScKCM-oplGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Fetch All Orders from the \"West\" Region"
      ],
      "metadata": {
        "id": "WK7BtIfstA3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "west_orders = collection.find({\"Region\": \"West\"})\n",
        "\n",
        "for order in west_orders:\n",
        "  print(order)"
      ],
      "metadata": {
        "id": "IdSEcmpXtFYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Find Orders Where Sales > 500"
      ],
      "metadata": {
        "id": "kZMUaR2ktaoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "high_sales = collection.find({\"Sales:\" {\"$gt\": 500}})\n",
        "\n",
        "for order in high_sales:\n",
        "  print(order)"
      ],
      "metadata": {
        "id": "EVLyMkTXtd-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Fetch Top 3 Orders with Highest Profit"
      ],
      "metadata": {
        "id": "TozRbCLGtuOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_profit_orders = collection.find().sort(\"Profit\", -1).limit(3)\n",
        "\n",
        "for order in top_profit_orders:\n",
        "  print(order)"
      ],
      "metadata": {
        "id": "SeC2ye7vt0zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Update Ship Mode from \"First Class\" to \"Premium Class\""
      ],
      "metadata": {
        "id": "rk7e7XLbuHSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = collection.update_many(\n",
        "    {\"Ship Mode\": \"First Class\"},\n",
        "    {\"$set\": {\"Ship Mode\": \"Premium Class\"}}\n",
        ")\n",
        "\n",
        "print(\"Documents updated:\", result.modified_count)"
      ],
      "metadata": {
        "id": "7bRwZcdJuJz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. 8. Delete All Orders Where Sales < 50"
      ],
      "metadata": {
        "id": "vD433tGlukzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delete_result = collection.delete_many({\"Sales\": {\"$lt\": 50}})\n",
        "\n",
        "print(\"Documents deleted:\", delete_result.deleted_count)"
      ],
      "metadata": {
        "id": "txtJB4QAuoIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Aggregation: Total Sales per Region"
      ],
      "metadata": {
        "id": "AZaIY2puu0bo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = [\n",
        "    {\n",
        "        \"$group\": {\n",
        "            \"_id\": \"$Region\",\n",
        "            \"Total_Sales\": {\"$sum\": \"$Sales\"}\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "for result in collection.aggregate(pipeline):\n",
        "    print(result)\n"
      ],
      "metadata": {
        "id": "j0OM-Hr1u24z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Fetch All Distinct Ship Mode Values"
      ],
      "metadata": {
        "id": "FHqToYJ4u5Kw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ship_modes = collection.distinct(\"Ship Mode\")\n",
        "print(\"Distinct Ship Modes:\", ship_modes)\n"
      ],
      "metadata": {
        "id": "7tOm3v0ju9f7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Count Number of Orders for Each Category"
      ],
      "metadata": {
        "id": "FytFCrcrvAJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = [\n",
        "    {\n",
        "        \"$group\": {\n",
        "            \"_id\": \"$Category\",\n",
        "            \"Order_Count\": {\"$sum\": 1}\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "for result in collection.aggregate(pipeline):\n",
        "    print(result)\n"
      ],
      "metadata": {
        "id": "24gS2CBSvCji"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}